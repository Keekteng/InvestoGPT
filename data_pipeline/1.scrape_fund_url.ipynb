{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import tz\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_date = (datetime.datetime.now(tz=tz.gettz('Asia/Singapore'))-timedelta(days=1)).strftime('%Y%m%d')\n",
    "urls_output_dir = f'../data/raw/{cur_date}/urls'\n",
    "funds_output_dir = f'../data/raw/{cur_date}/funds'\n",
    "\n",
    "os.makedirs(urls_output_dir, exist_ok=True)\n",
    "os.makedirs(funds_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium stealth driver used for scraping Google Scholar\n",
    "def create_driver(debug=True):\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    if debug==False:\n",
    "        options.add_argument(\"--headless\")\n",
    "    # options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    # options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = uc.Chrome(\n",
    "        options=options\n",
    "    )\n",
    "    stealth(driver,\n",
    "            # user_agent=agent,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Fund URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_fund_url(output_dir):\n",
    "    url = 'https://investor.vanguard.com/investment-products/list/all'\n",
    "    driver = create_driver()\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(3,5))\n",
    "    nxt_button = True\n",
    "    page_count = 0\n",
    "\n",
    "    try:\n",
    "        fund_urls = {}\n",
    "        while nxt_button:\n",
    "            page_count+=1\n",
    "            table = driver.find_element(By.XPATH,\"//div[@class='col-md-9 col-sm-12']\")\n",
    "            table_body = table.find_element(By.TAG_NAME,'tbody')\n",
    "            table_rows = table_body.find_elements(By.TAG_NAME,'tr')\n",
    "            for row in table_rows:\n",
    "\n",
    "                symbol = row.find_element(By.TAG_NAME,'span').text\n",
    "                fund_url = row.find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "                fund_urls[symbol] = fund_url\n",
    "            button = driver.find_element(By.ID,'next-page-btn')\n",
    "            button.location_once_scrolled_into_view\n",
    "            time.sleep(random.uniform(1,2))\n",
    "            if button.find_element(By.TAG_NAME,'vui-icon').get_attribute('class') == 'disabled':\n",
    "                nxt_button=False\n",
    "            else:\n",
    "                button.click()\n",
    "                time.sleep(random.uniform(1,2))\n",
    "        driver.quit()\n",
    "        output_file = output_dir+'/funds_url.json'\n",
    "        with open(output_file,'w') as f:\n",
    "            json.dump(fund_urls,f)\n",
    "    except:\n",
    "        print(symbol,page_count)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_fund_url(output_dir=urls_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Fund Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fund_details(html_page_source,fund_detail,symbol,driver):\n",
    "    dashboard = html_page_source.find(name='section',attrs={'id':'Dashboard'})\n",
    "    if dashboard:\n",
    "        fund_name = dashboard.find(name='h1',attrs={'class':'fund-name rps-display-two'}).text\n",
    "        fund_detail['name'] = fund_name\n",
    "    \n",
    "    overview = html_page_source.find(name='div',attrs={'id':'overview_section'})\n",
    "    if overview:\n",
    "        key_fact_table = overview.find(name='table',attrs={'class':'table key-fact-table'})\n",
    "        if key_fact_table:\n",
    "            fund_detail['key_fact_table'] = {}\n",
    "            for row in key_fact_table.find('tbody').find_all('tr'):\n",
    "                key = row.find('th').text\n",
    "                value = row.find('td').text\n",
    "                fund_detail['key_fact_table'][key] = value\n",
    "\n",
    "        reward_scale = overview.find(name='div',attrs={'class':\"reward--scale__item reward--scale__item--active4 ng-star-inserted\"})\n",
    "        if reward_scale:\n",
    "            span_element = reward_scale.find(name='span',attrs={'aria-label':\"The fund risk level is a\"})\n",
    "            next_sibling = span_element.find_next_sibling()\n",
    "            fund_detail['risk_level'] = next_sibling.text\n",
    "        \n",
    "        ppf_container = overview.find(name='div',attrs={'class':'col-lg-4 col-sm-12 ppf-container'})\n",
    "        if ppf_container:\n",
    "            ytd_percent = ppf_container.find(name='h4',attrs={'data-rpa-tag-id':'ytdPercent'})\n",
    "            ytd_percent_market = ppf_container.find(name='h4',attrs={'data-rpa-tag-id':'ytdPercentMarket'})\n",
    "            ytd_percent_nav = ppf_container.find(name='h4',attrs={'data-rpa-tag-id':'ytdPercentNAV'})\n",
    "            if ytd_percent:\n",
    "                fund_detail['ytd_returns'] = ytd_percent.text\n",
    "            if ytd_percent_market:\n",
    "                fund_detail['ytd_market_returns'] = ytd_percent_market.text\n",
    "            if ytd_percent_nav:\n",
    "                fund_detail['ytd_nav_returns'] = ytd_percent_nav.text\n",
    "        \n",
    "        product_summary = overview.find(name='div',attrs={'data-rpa-tag-id':'productSummary'})\n",
    "        if product_summary:\n",
    "            if product_summary.find('p'):\n",
    "                fund_detail['product_summary'] = product_summary.find('p').text\n",
    "            elif product_summary.find_all('li'):\n",
    "                summary_text = \"\"\n",
    "                for li in product_summary.find_all('li'):\n",
    "                    summary_text = summary_text+li.text\n",
    "                fund_detail['product_summary'] = summary_text\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        min_inv = overview.find(name='h4',attrs={'data-rpa-tag-id':'minInvestment'})\n",
    "        if min_inv:\n",
    "            fund_detail['min_investment'] = min_inv.text\n",
    "        \n",
    "        exp_ratio = overview.find(name='h4',attrs={'data-rpa-tag-id':'expenseRatio'})\n",
    "        if exp_ratio:\n",
    "            fund_detail['exp_ratio'] = exp_ratio.text\n",
    "        \n",
    "    perf = html_page_source.find(name='div',attrs={'id':'performance-fees_section'})\n",
    "    if perf:\n",
    "        perf_table = perf.find(name='table',attrs={'aria-label':'Performance summary'})\n",
    "        if perf_table:\n",
    "            fund_detail['perf_table'] = {'index':['Month-end','3-Month total','YTD','1-yr','3-yr','5-yr','10-yr','Since inception']}\n",
    "            for row in perf_table.find('tbody').find_all('tr'):\n",
    "                values = []\n",
    "                for td in row.find_all('td'):\n",
    "                    values.append(td.text)\n",
    "                fund_detail['perf_table'][row.find('th').text] = values\n",
    "    \n",
    "    fund_detail['historical_price_table'] = {}\n",
    "    if html_page_source.find(name='div',attrs={'id':'mat-tab-label-0-3'}):\n",
    "        tab_1_year = driver.find_element(By.ID,'mat-tab-label-0-3')    \n",
    "        tab_1_year.location_once_scrolled_into_view\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        tab_1_year.click()\n",
    "        \n",
    "        time.sleep(random.uniform(2,4))\n",
    "        radio_button = driver.find_element(By.XPATH,\"//input[@type='radio' and @aria-label='table']\")\n",
    "        radio_button.click()\n",
    "        time.sleep(random.uniform(2,4))\n",
    "\n",
    "        listbox_location = driver.find_element(By.XPATH,\"//button[@tabindex='0' and @aria-haspopup='listbox']\")\n",
    "        dates = []\n",
    "        daily_prices = []\n",
    "        listbox_location.click()\n",
    "        time.sleep(random.uniform(0,0.5))\n",
    "\n",
    "        overlay_container = driver.find_element(By.XPATH,\"//div[@class='cdk-overlay-container']\")\n",
    "        driver.find_element(By.XPATH,\"//table[@aria-label='Historical prices table']\").location_once_scrolled_into_view\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        listbox = overlay_container.find_element(By.XPATH,\"//div[@role='listbox' and @tabindex='0']\")\n",
    "        num_options = len(listbox.find_elements(By.TAG_NAME,'vui-option'))\n",
    "        for index,option in enumerate(listbox.find_elements(By.TAG_NAME,'vui-option')):\n",
    "            option.location_once_scrolled_into_view\n",
    "            time.sleep(random.uniform(0.5,1))\n",
    "            option.click()\n",
    "            time.sleep(random.uniform(0.5,1))\n",
    "            updated_page_source = BeautifulSoup(driver.page_source)\n",
    "            price = updated_page_source.find(name='div',attrs={'id':'price_section'})\n",
    "            for tr in price.find(name='table',attrs={'aria-label':'Historical prices table'}).find('tbody').find_all('tr'):\n",
    "                dates.append(tr.find(name='td',attrs={'data-rpa-tag-id':'historicalDate'}).text)\n",
    "                daily_prices.append(tr.find(name='td',attrs={'data-rpa-tag-id':'historicalPrice'}).text)\n",
    "\n",
    "            if index == num_options-1:\n",
    "                break\n",
    "            listbox_location.click()\n",
    "            time.sleep(random.uniform(0.5,1))\n",
    "\n",
    "        fund_detail['historical_price_table']['date'] = dates\n",
    "        fund_detail['historical_price_table']['price'] = daily_prices\n",
    "\n",
    "    fundamental_table = html_page_source.find(name='div',attrs={'id':'characteristics-tabset'})\n",
    "    fund_detail['portfolio_fundamental_table'] = {}\n",
    "    if fundamental_table:\n",
    "        trs = fundamental_table.find_all('tr')\n",
    "        fundamental_list = []\n",
    "        fund_list = []\n",
    "        benchmark_list = []\n",
    "        for tr in trs[1:]:\n",
    "            tds = tr.find_all('td')\n",
    "            for index,td in enumerate(tds):\n",
    "                if index==0:\n",
    "                    fundamental_list.append(td.text)\n",
    "                elif index==1:\n",
    "                    fund_list.append(td.text)\n",
    "                else:\n",
    "                    benchmark_list.append(td.text)\n",
    "        fund_detail['portfolio_fundamental_table']['fundamentals'] = fundamental_list\n",
    "        fund_detail['portfolio_fundamental_table'][symbol] = fund_list\n",
    "        fund_detail['portfolio_fundamental_table']['benchmark'] = benchmark_list\n",
    "    \n",
    "\n",
    "    fund_detail['weighted_exposure_table'] = {}\n",
    "    if html_page_source.find(name='vui-tab-group',attrs={'id':'weighting-prices-tabset'}):\n",
    "        tab_list = driver.find_element(By.ID,'weighting-prices-tabset').find_element(By.CSS_SELECTOR,'.vui-tabs-container.vui-tab-not-contained-style').find_elements(By.TAG_NAME,'button')\n",
    "        driver.find_element(By.XPATH,\"//exemplar-weighting-exposure\").location_once_scrolled_into_view\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        for button in tab_list:\n",
    "            exposure_type = BeautifulSoup(button.get_attribute('outerHTML'),'html.parser').find('div').text\n",
    "            button.click()\n",
    "            time.sleep(random.uniform(1,2))\n",
    "            updated_page_source = BeautifulSoup(driver.page_source)\n",
    "            asset_allocation_table = updated_page_source.find(name='mat-table',attrs={'id':'assetAllocationTable'})\n",
    "            rows = asset_allocation_table.find_all(name='mat-row',attrs={'role':'row'})\n",
    "            exposure_type_list = []\n",
    "            symbol_list = []\n",
    "            benchmark_list = []\n",
    "            weight_difference_list = []\n",
    "            for row in rows:\n",
    "                for cell in row.find_all(name='mat-cell'):\n",
    "                    span = cell.find('span')\n",
    "                    if span:\n",
    "                        data_rpa_tag_id = span.get('data-rpa-tag-id')\n",
    "                        if data_rpa_tag_id==\"weightedExposuresTab\":\n",
    "                            exposure_type_list.append(span.text)\n",
    "                        \n",
    "                        if data_rpa_tag_id=='symbol_LongPct':\n",
    "                            symbol_list.append(span.text)\n",
    "                        \n",
    "                        if data_rpa_tag_id=='benchmark_ShortPct':\n",
    "                            benchmark_list.append(span.text)\n",
    "                        \n",
    "                        if data_rpa_tag_id=='weightPct':\n",
    "                            weight_difference_list.append(span.text)\n",
    "\n",
    "            fund_detail['weighted_exposure_table'][exposure_type] = {}\n",
    "            fund_detail['weighted_exposure_table'][exposure_type][exposure_type] = exposure_type_list\n",
    "            fund_detail['weighted_exposure_table'][exposure_type][symbol] = symbol_list\n",
    "            fund_detail['weighted_exposure_table'][exposure_type][\"Benchmark\"] = benchmark_list\n",
    "            fund_detail['weighted_exposure_table'][exposure_type]['+/-Weight'] = weight_difference_list\n",
    "    \n",
    "    return fund_detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_and_risk(html_page_source,fund_detail,symbol,driver):\n",
    "    \n",
    "    dashboard = html_page_source.find(name='section',attrs={'id':'Dashboard'})\n",
    "    if dashboard:\n",
    "        fund_name = dashboard.find(name='h1',attrs={'class':'fund-name rps-display-two'}).text\n",
    "        fund_detail['name'] = fund_name\n",
    "    \n",
    "    overview = html_page_source.find(name='div',attrs={'id':'overview_section'})\n",
    "    if overview:\n",
    "        reward_scale = overview.find(name='div',attrs={'class':\"reward--scale__item reward--scale__item--active4 ng-star-inserted\"})\n",
    "        if reward_scale:\n",
    "            span_element = reward_scale.find(name='span',attrs={'aria-label':\"The fund risk level is a\"})\n",
    "            next_sibling = span_element.find_next_sibling()\n",
    "            fund_detail['risk_level'] = next_sibling.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/raw/20231212/urls/funds_url.json','r')as f:\n",
    "    funds_url = json.load(f)\n",
    "\n",
    "try:\n",
    "    for index,(symbol,url) in enumerate(funds_url.items()):\n",
    "        # if index<349:\n",
    "        #     continue\n",
    "        driver = create_driver()\n",
    "        fund_detail = {}\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(1,1.5))\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        fund_detail = extract_fund_details(soup,fund_detail,symbol,driver)\n",
    "        with open(f\"{funds_output_dir}/{symbol}.json\",'w') as f:\n",
    "            json.dump(fund_detail,f)\n",
    "        driver.quit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR --> {symbol}, Index --> {index}\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InvestoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
